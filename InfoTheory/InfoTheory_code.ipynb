{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information-Theoretic Analysis: Macro-Finance Causality\n",
    "\n",
    "Research Framework:\n",
    "1. Data Preparation: Load, clean, align time series, discretize\n",
    "2. Information Measures: MI, TE, CTE using JIDT\n",
    "3. Statistical Significance: Permutation test (p < 0.05)\n",
    "4. Temporal Analysis: Rolling window to capture dynamics\n",
    "\n",
    "Core Measures:\n",
    "- Mutual Information (MI): Total dependence (baseline)\n",
    "- Transfer Entropy (TE): Directed information flow\n",
    "- Conditional Transfer Entropy (CTE): Net causal effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jpype\n",
    "from jpype import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: JIDT Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jarLocation = r\"C:\\Users\\11727\\Desktop\\5030\\infodynamics-dist-1.6.1 (2)\\infodynamics.jar\"\n",
    "if not isJVMStarted():\n",
    "    startJVM(getDefaultJVMPath(), \"-ea\", \"-Djava.class.path=\" + jarLocation)\n",
    "\n",
    "# Import JIDT calculators\n",
    "MI_D = JPackage(\"infodynamics.measures.discrete\").MutualInformationCalculatorDiscrete\n",
    "CMI_D = JPackage(\"infodynamics.measures.discrete\").ConditionalMutualInformationCalculatorDiscrete\n",
    "TE_D = JPackage(\"infodynamics.measures.discrete\").TransferEntropyCalculatorDiscrete\n",
    "\n",
    "# Helper function: convert numpy array to Java int array\n",
    "def ja_int(x):\n",
    "    \"\"\"Convert numpy array to Java int array for JIDT\"\"\"\n",
    "    x = np.asarray(x).astype(np.int32).ravel()\n",
    "    return JArray(JInt)(x.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"Load, merge, and preprocess financial and macroeconomic data\"\"\"\n",
    "    # Load data\n",
    "    stock_data = pd.read_csv('sp500_data.csv', parse_dates=['Date'], index_col='Date')\n",
    "    macro_data = pd.read_csv('macro_data.csv', parse_dates=['Date'], index_col='Date')\n",
    "    \n",
    "    # Align by month\n",
    "    stock_data['YearMonth'] = stock_data.index.to_period('M')\n",
    "    macro_data['YearMonth'] = macro_data.index.to_period('M')\n",
    "    data = pd.merge(stock_data.set_index('YearMonth'), macro_data.set_index('YearMonth'), \n",
    "                   left_index=True, right_index=True, how='inner')\n",
    "    \n",
    "    # Calculate log returns\n",
    "    data = data[data['Stock'] > 0].copy()\n",
    "    data['Stock_Log_Return'] = np.log(data['Stock'] / data['Stock'].shift(1))\n",
    "    \n",
    "    # Calculate inflation if needed\n",
    "    if 'Inflation' not in data.columns and 'CPI' in data.columns:\n",
    "        data['Inflation'] = data['CPI'].pct_change(12) * 100\n",
    "    \n",
    "    # Clean and set index\n",
    "    data = data.dropna(subset=['Stock_Log_Return', 'Interest_Rate']).copy()\n",
    "    data.index = data.index.to_timestamp('M')\n",
    "    \n",
    "    return data.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discretization (Required for JIDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(x, n_bins=10):\n",
    "    \"\"\"Discretize continuous variable for JIDT (requires discrete data)\"\"\"\n",
    "    x_clean = pd.to_numeric(x, errors='coerce').dropna()\n",
    "    if len(x_clean) < 10:\n",
    "        return pd.Series(index=x.index, dtype=float)\n",
    "    \n",
    "    n_bins_actual = min(n_bins, len(x_clean) // 2)\n",
    "    bins = pd.qcut(x_clean, q=n_bins_actual, labels=False, duplicates='drop')\n",
    "    result = pd.Series(index=x.index, dtype=float)\n",
    "    result[x_clean.index] = bins\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Information-Theoretic Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transfer_entropy_for_arrays(x, y, lag=1):\n",
    "    \"\"\"Compute Transfer Entropy for numpy arrays (used in permutation test)\"\"\"\n",
    "    try:\n",
    "        if len(x) != len(y) or len(x) < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        x_bin = discretize(pd.Series(x))\n",
    "        y_bin = discretize(pd.Series(y))\n",
    "        mask = ~(x_bin.isna() | y_bin.isna())\n",
    "        x_int = x_bin[mask].astype(int).values\n",
    "        y_int = y_bin[mask].astype(int).values\n",
    "        \n",
    "        if len(x_int) < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        base = max(x_int.max(), y_int.max()) + 1\n",
    "        calc = TE_D(base, 1, 1, 1, 1, lag)\n",
    "        calc.initialise()\n",
    "        calc.addObservations(ja_int(x_int), ja_int(y_int))\n",
    "        return float(calc.computeAverageLocalOfObservations())\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi(data, x_col, y_col):\n",
    "    \"\"\"Compute Mutual Information (baseline: total dependence)\"\"\"\n",
    "    try:\n",
    "        x_bin = discretize(data[x_col])\n",
    "        y_bin = discretize(data[y_col])\n",
    "        mask = ~(x_bin.isna() | y_bin.isna())\n",
    "        if mask.sum() < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        x_int = x_bin[mask].astype(int).values\n",
    "        y_int = y_bin[mask].astype(int).values\n",
    "        base = max(x_int.max(), y_int.max()) + 1\n",
    "        \n",
    "        calc = MI_D()\n",
    "        calc.initialise(base, base, 1)\n",
    "        calc.addObservations(ja_int(x_int), ja_int(y_int))\n",
    "        return float(calc.computeAverageLocalOfObservations())\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_te(data, source_col, target_col, lag=1):\n",
    "    \"\"\"Compute Transfer Entropy (directed information flow)\"\"\"\n",
    "    try:\n",
    "        source_bin = discretize(data[source_col])\n",
    "        target_bin = discretize(data[target_col])\n",
    "        mask = ~(source_bin.isna() | target_bin.isna())\n",
    "        if mask.sum() < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        source_int = source_bin[mask].astype(int).values\n",
    "        target_int = target_bin[mask].astype(int).values\n",
    "        base = max(source_int.max(), target_int.max()) + 1\n",
    "        \n",
    "        calc = TE_D(base, 1, 1, 1, 1, lag)\n",
    "        calc.initialise()\n",
    "        calc.addObservations(ja_int(source_int), ja_int(target_int))\n",
    "        return float(calc.computeAverageLocalOfObservations())\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_cte(data, source_col, target_col, condition_col, lag=1):\n",
    "    \"\"\"Compute Conditional Transfer Entropy (net causal effect)\"\"\"\n",
    "    try:\n",
    "        source_bin = discretize(data[source_col])\n",
    "        target_bin = discretize(data[target_col])\n",
    "        condition_bin = discretize(data[condition_col])\n",
    "        mask = ~(source_bin.isna() | target_bin.isna() | condition_bin.isna())\n",
    "        if mask.sum() < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        source_int = source_bin[mask].astype(int).values\n",
    "        target_int = target_bin[mask].astype(int).values\n",
    "        condition_int = condition_bin[mask].astype(int).values\n",
    "        \n",
    "        bx, by, bz = source_int.max() + 1, target_int.max() + 1, condition_int.max() + 1\n",
    "        calc = CMI_D()\n",
    "        calc.initialise(bx, by, bz)\n",
    "        calc.addObservations(ja_int(source_int), ja_int(target_int), ja_int(condition_int))\n",
    "        return float(calc.computeAverageLocalOfObservations())\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def te_with_pvalue(data, source_col, target_col, lag=1, n_perm=200):\n",
    "    \"\"\"\n",
    "    Compute Transfer Entropy with permutation test for statistical significance.\n",
    "    \n",
    "    Null hypothesis: Random permutation of source (preserves target structure).\n",
    "    P-value formula: (count(TE_perm >= TE_obs) + 1) / (n_perm + 1)\n",
    "    Significance threshold: p < 0.05\n",
    "    \"\"\"\n",
    "    # Compute observed TE\n",
    "    te_obs = compute_te(data, source_col, target_col, lag)\n",
    "    if np.isnan(te_obs):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Extract aligned arrays\n",
    "    common_idx = data[[source_col, target_col]].dropna().index\n",
    "    if len(common_idx) < 10:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    x = data.loc[common_idx, source_col].values\n",
    "    y = data.loc[common_idx, target_col].values\n",
    "    \n",
    "    # Permutation test: shuffle source only (preserve target structure)\n",
    "    te_perm = []\n",
    "    for _ in range(n_perm):\n",
    "        x_perm = np.random.permutation(x)\n",
    "        temp_df = pd.DataFrame({source_col: x_perm, target_col: y}, index=common_idx)\n",
    "        te_val = compute_te(temp_df, source_col, target_col, lag)\n",
    "        if not np.isnan(te_val):\n",
    "            te_perm.append(te_val)\n",
    "    \n",
    "    if len(te_perm) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Calculate p-value (right-tailed test)\n",
    "    p_value = (np.sum(np.array(te_perm) >= te_obs) + 1) / (n_perm + 1)\n",
    "    return te_obs, p_value\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rolling Window Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_analysis(data, window_size=60, n_perm=200):\n",
    "    \"\"\"\n",
    "    Rolling window analysis with bidirectional TE and significance testing.\n",
    "    Each window computes: TE, p-values, MI, and CTE.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'dates': [],\n",
    "        'te_m2s': [],  # Macro to Stock\n",
    "        'te_s2m': [],  # Stock to Macro\n",
    "        'p_m2s': [],\n",
    "        'p_s2m': [],\n",
    "        'mi': [],\n",
    "        'cte': []\n",
    "    }\n",
    "    \n",
    "    n_windows = len(data) - window_size + 1\n",
    "    print(f\"Computing {n_windows} rolling windows (n_perm={n_perm})...\")\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        window = data.iloc[i:i+window_size]\n",
    "        \n",
    "        # Bidirectional TE with significance\n",
    "        te_m2s, p_m2s = te_with_pvalue(window, 'Interest_Rate', 'Stock_Log_Return', lag=1, n_perm=n_perm)\n",
    "        te_s2m, p_s2m = te_with_pvalue(window, 'Stock_Log_Return', 'Interest_Rate', lag=1, n_perm=n_perm)\n",
    "        \n",
    "        # Mutual Information (baseline)\n",
    "        mi = compute_mi(window, 'Interest_Rate', 'Stock_Log_Return')\n",
    "        \n",
    "        # Conditional TE (if inflation available)\n",
    "        if 'Inflation' in window.columns and window['Inflation'].notna().sum() >= 10:\n",
    "            cte = compute_cte(window, 'Interest_Rate', 'Stock_Log_Return', 'Inflation', lag=1)\n",
    "        else:\n",
    "            cte = np.nan\n",
    "        \n",
    "        results['dates'].append(window.index[-1])\n",
    "        results['te_m2s'].append(te_m2s)\n",
    "        results['te_s2m'].append(te_s2m)\n",
    "        results['p_m2s'].append(p_m2s)\n",
    "        results['p_s2m'].append(p_s2m)\n",
    "        results['mi'].append(mi)\n",
    "        results['cte'].append(cte)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Processed {i+1}/{n_windows} windows...\")\n",
    "    \n",
    "    print(f\"Analysis complete: {n_windows} windows\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    \"\"\"Visualize analysis results\"\"\"\n",
    "    dates = results['dates']\n",
    "    te_m2s = np.array(results['te_m2s'])\n",
    "    te_s2m = np.array(results['te_s2m'])\n",
    "    p_m2s = np.array(results['p_m2s'])\n",
    "    p_s2m = np.array(results['p_s2m'])\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Bidirectional TE (with significance markers)\n",
    "    sig_m2s = p_m2s < 0.05\n",
    "    sig_s2m = p_s2m < 0.05\n",
    "    \n",
    "    axes[0].plot(dates, te_m2s, 'b-', alpha=0.7, label='TE (Macro→Stock)')\n",
    "    axes[0].plot(dates, te_s2m, 'orange', linestyle='-', alpha=0.7, label='TE (Stock→Macro)')\n",
    "    axes[0].scatter(np.array(dates)[sig_m2s], te_m2s[sig_m2s], c='blue', marker='*', s=50, \n",
    "                   label='Significant (p<0.05)', zorder=5)\n",
    "    axes[0].scatter(np.array(dates)[sig_s2m], te_s2m[sig_s2m], c='orange', marker='*', s=50, zorder=5)\n",
    "    axes[0].set_title('Bidirectional Transfer Entropy (p < 0.05 significance)', fontweight='bold')\n",
    "    axes[0].set_ylabel('TE (bits)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Conditional TE vs Unconditional TE\n",
    "    axes[1].plot(dates, results['cte'], 'r-', label='Conditional TE', alpha=0.8)\n",
    "    axes[1].plot(dates, te_m2s, 'b--', label='Unconditional TE', alpha=0.6)\n",
    "    axes[1].set_title('Conditional Transfer Entropy (Net Effect)', fontweight='bold')\n",
    "    axes[1].set_ylabel('CTE/TE (bits)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. TE Difference (dominant direction)\n",
    "    te_diff = te_m2s - te_s2m\n",
    "    axes[2].plot(dates, te_diff, 'g-', alpha=0.8)\n",
    "    axes[2].axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[2].fill_between(dates, 0, te_diff, where=(te_diff > 0), alpha=0.3, color='blue', label='Macro→Stock')\n",
    "    axes[2].fill_between(dates, 0, te_diff, where=(te_diff < 0), alpha=0.3, color='orange', label='Stock→Macro')\n",
    "    axes[2].set_title('TE Difference (Dominant Direction)', fontweight='bold')\n",
    "    axes[2].set_ylabel('TE Difference (bits)')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. P-values\n",
    "    axes[3].semilogy(dates, p_m2s, 'b-', alpha=0.7, label='p-value (Macro→Stock)')\n",
    "    axes[3].semilogy(dates, p_s2m, 'orange', linestyle='-', alpha=0.7, label='p-value (Stock→Macro)')\n",
    "    axes[3].axhline(0.05, color='r', linestyle='--', linewidth=2, label='p = 0.05')\n",
    "    axes[3].set_title('P-values (Permutation Test)', fontweight='bold')\n",
    "    axes[3].set_ylabel('P-value (log scale)')\n",
    "    axes[3].set_xlabel('Date')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    axes[3].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== Summary Statistics ===\")\n",
    "    print(f\"TE (Macro→Stock): mean={np.nanmean(te_m2s):.4f}, std={np.nanstd(te_m2s):.4f}\")\n",
    "    print(f\"TE (Stock→Macro): mean={np.nanmean(te_s2m):.4f}, std={np.nanstd(te_s2m):.4f}\")\n",
    "    print(f\"Mutual Information: mean={np.nanmean(results['mi']):.4f}\")\n",
    "    print(f\"Conditional TE: mean={np.nanmean(results['cte']):.4f}\")\n",
    "    \n",
    "    valid_p = ~(np.isnan(p_m2s) | np.isnan(p_s2m))\n",
    "    if valid_p.sum() > 0:\n",
    "        sig_m2s_count = (p_m2s[valid_p] < 0.05).sum()\n",
    "        sig_s2m_count = (p_s2m[valid_p] < 0.05).sum()\n",
    "        print(f\"\\nSignificant windows (p < 0.05):\")\n",
    "        print(f\"  Macro→Stock: {sig_m2s_count}/{valid_p.sum()} ({100*sig_m2s_count/valid_p.sum():.1f}%)\")\n",
    "        print(f\"  Stock→Macro: {sig_s2m_count}/{valid_p.sum()} ({100*sig_s2m_count/valid_p.sum():.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7. Main Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_analysis_bidirectional(data, window_size=60, n_perm=200):\n",
    "    te_macro_to_stock = []\n",
    "    te_stock_to_macro = []\n",
    "    p_macro_to_stock = []\n",
    "    p_stock_to_macro = []\n",
    "    cte_results = []\n",
    "    mi_results = []\n",
    "    dates = []\n",
    "    \n",
    "    if len(data) < window_size:\n",
    "        print(f\"Warning: Insufficient data ({len(data)} < {window_size})\")\n",
    "        return [], [], [], [], [], [], []\n",
    "    \n",
    "    print(f\"Computing TE with permutation test (n_perm={n_perm}) for {len(data) - window_size + 1} windows...\")\n",
    "    for start in range(len(data) - window_size + 1):\n",
    "        window_data = data.iloc[start:start+window_size]\n",
    "        \n",
    "        # Bidirectional TE with permutation test (Question 1)\n",
    "        # Use compute_te_with_pvalue for statistical significance testing\n",
    "        te_m2s, p_m2s = compute_te_with_pvalue(\n",
    "            window_data, 'Interest_Rate', 'Stock_Log_Return', lag=1, n_perm=n_perm\n",
    "        )\n",
    "        te_s2m, p_s2m = compute_te_with_pvalue(\n",
    "            window_data, 'Stock_Log_Return', 'Interest_Rate', lag=1, n_perm=n_perm\n",
    "        )\n",
    "        \n",
    "        # Conditional TE (Question 2)\n",
    "        if 'Inflation' in window_data.columns and window_data['Inflation'].notna().sum() >= 10:\n",
    "            cte = compute_conditional_transfer_entropy(\n",
    "                window_data, 'Interest_Rate', 'Stock_Log_Return', 'Inflation', lag=1\n",
    "            )\n",
    "        else:\n",
    "            cte = np.nan\n",
    "        \n",
    "        # Mutual Information (baseline)\n",
    "        mi = compute_mutual_information(window_data, 'Interest_Rate', 'Stock_Log_Return')\n",
    "        \n",
    "        te_macro_to_stock.append(te_m2s)\n",
    "        te_stock_to_macro.append(te_s2m)\n",
    "        p_macro_to_stock.append(p_m2s)\n",
    "        p_stock_to_macro.append(p_s2m)\n",
    "        cte_results.append(cte)\n",
    "        mi_results.append(mi)\n",
    "        dates.append(window_data.index[-1])\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (start + 1) % 50 == 0:\n",
    "            print(f\"  Processed {start + 1}/{len(data) - window_size + 1} windows...\")\n",
    "    \n",
    "    print(f\"\\nRolling window analysis complete: {len(te_macro_to_stock)} windows\")\n",
    "    if dates:\n",
    "        te_m2s_arr = np.array(te_macro_to_stock)\n",
    "        te_s2m_arr = np.array(te_stock_to_macro)\n",
    "        p_m2s_arr = np.array(p_macro_to_stock)\n",
    "        p_s2m_arr = np.array(p_stock_to_macro)\n",
    "        \n",
    "        # Valid windows (non-NaN TE values)\n",
    "        mask = ~(np.isnan(te_m2s_arr) | np.isnan(te_s2m_arr))\n",
    "        if mask.sum() > 0:\n",
    "            macro_dominates = (te_m2s_arr[mask] > te_s2m_arr[mask]).sum()\n",
    "            print(f\"  Dominant direction: Macro→Stock in {macro_dominates}/{mask.sum()} windows ({100*macro_dominates/mask.sum():.1f}%)\")\n",
    "            \n",
    "            # Significance statistics\n",
    "            sig_m2s = (p_m2s_arr[mask] < 0.05).sum()\n",
    "            sig_s2m = (p_s2m_arr[mask] < 0.05).sum()\n",
    "            print(f\"  Significant TE (p < 0.05): Macro→Stock: {sig_m2s}/{mask.sum()} ({100*sig_m2s/mask.sum():.1f}%), Stock→Macro: {sig_s2m}/{mask.sum()} ({100*sig_s2m/mask.sum():.1f}%)\")\n",
    "    \n",
    "    return te_macro_to_stock, te_stock_to_macro, p_macro_to_stock, p_stock_to_macro, cte_results, mi_results, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_bidirectional(te_macro_to_stock, te_stock_to_macro, cte_results, mi_results, dates=None, \n",
    "                               p_macro_to_stock=None, p_stock_to_macro=None):\n",
    "    \"\"\"Plot bidirectional TE analysis results (Question 1-3) with optional p-values\"\"\"\n",
    "    # Determine number of subplots (5 if p-values provided, 4 otherwise)\n",
    "    n_plots = 5 if p_macro_to_stock is not None else 4\n",
    "    fig, axes = plt.subplots(n_plots, 1, figsize=(14, 2.5 * n_plots))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    x_axis = dates if dates is not None else range(len(te_macro_to_stock))\n",
    "    \n",
    "    # 1. Bidirectional TE (Question 1)\n",
    "    # Plot with different styles for significant vs non-significant windows\n",
    "    if p_macro_to_stock is not None:\n",
    "        p_m2s_arr = np.array(p_macro_to_stock)\n",
    "        p_s2m_arr = np.array(p_stock_to_macro)\n",
    "        sig_mask_m2s = p_m2s_arr < 0.05\n",
    "        sig_mask_s2m = p_s2m_arr < 0.05\n",
    "        non_sig_mask_m2s = ~sig_mask_m2s & ~np.isnan(p_m2s_arr)\n",
    "        non_sig_mask_s2m = ~sig_mask_s2m & ~np.isnan(p_s2m_arr)\n",
    "        \n",
    "        # Plot significant windows with solid lines\n",
    "        if sig_mask_m2s.any():\n",
    "            axes[0].plot(np.array(x_axis)[sig_mask_m2s], np.array(te_macro_to_stock)[sig_mask_m2s], \n",
    "                        color='blue', linestyle='-', linewidth=2, alpha=0.9, label='TE (Macro→Stock, p<0.05)', zorder=3)\n",
    "        if sig_mask_s2m.any():\n",
    "            axes[0].plot(np.array(x_axis)[sig_mask_s2m], np.array(te_stock_to_macro)[sig_mask_s2m], \n",
    "                        color='orange', linestyle='-', linewidth=2, alpha=0.9, label='TE (Stock→Macro, p<0.05)', zorder=3)\n",
    "        \n",
    "        # Plot non-significant windows with dashed lines and lower alpha\n",
    "        if non_sig_mask_m2s.any():\n",
    "            axes[0].plot(np.array(x_axis)[non_sig_mask_m2s], np.array(te_macro_to_stock)[non_sig_mask_m2s], \n",
    "                        color='blue', linestyle='--', linewidth=1, alpha=0.4, label='TE (Macro→Stock, p≥0.05)', zorder=2)\n",
    "        if non_sig_mask_s2m.any():\n",
    "            axes[0].plot(np.array(x_axis)[non_sig_mask_s2m], np.array(te_stock_to_macro)[non_sig_mask_s2m], \n",
    "                        color='orange', linestyle='--', linewidth=1, alpha=0.4, label='TE (Stock→Macro, p≥0.05)', zorder=2)\n",
    "        \n",
    "        # Handle NaN windows\n",
    "        nan_mask_m2s = np.isnan(p_m2s_arr)\n",
    "        nan_mask_s2m = np.isnan(p_s2m_arr)\n",
    "        if nan_mask_m2s.any():\n",
    "            axes[0].plot(np.array(x_axis)[nan_mask_m2s], np.array(te_macro_to_stock)[nan_mask_m2s], \n",
    "                        color='gray', linestyle=':', linewidth=0.5, alpha=0.3, label='TE (Macro→Stock, NaN)', zorder=1)\n",
    "        if nan_mask_s2m.any():\n",
    "            axes[0].plot(np.array(x_axis)[nan_mask_s2m], np.array(te_stock_to_macro)[nan_mask_s2m], \n",
    "                        color='gray', linestyle=':', linewidth=0.5, alpha=0.3, label='TE (Stock→Macro, NaN)', zorder=1)\n",
    "    else:\n",
    "        # Fallback if no p-values provided\n",
    "        axes[0].plot(x_axis, te_macro_to_stock, label='TE (Macro→Stock)', color='blue', alpha=0.8)\n",
    "        axes[0].plot(x_axis, te_stock_to_macro, label='TE (Stock→Macro)', color='orange', alpha=0.8)\n",
    "    \n",
    "    axes[0].set_title('Bidirectional Transfer Entropy (Question 1)\\n判定标准: p < 0.05 (permutation test, n_perm=200)', fontweight='bold')\n",
    "    axes[0].set_ylabel('TE (bits)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend(fontsize=8, loc='best')\n",
    "    \n",
    "    # 2. Conditional TE (Question 2)\n",
    "    axes[1].plot(x_axis, cte_results, label='Conditional TE', color='red', alpha=0.8)\n",
    "    axes[1].plot(x_axis, te_macro_to_stock, label='Unconditional TE', color='blue', linestyle=':', alpha=0.6)\n",
    "    axes[1].set_title('Conditional Transfer Entropy: Net Effect (Question 2)', fontweight='bold')\n",
    "    axes[1].set_ylabel('CTE/TE (bits)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # 3. TE Difference\n",
    "    te_diff = np.array(te_macro_to_stock) - np.array(te_stock_to_macro)\n",
    "    axes[2].plot(x_axis, te_diff, label='TE Difference', color='green', alpha=0.8)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[2].fill_between(x_axis, 0, te_diff, where=(te_diff > 0), alpha=0.3, color='blue')\n",
    "    axes[2].fill_between(x_axis, 0, te_diff, where=(te_diff < 0), alpha=0.3, color='orange')\n",
    "    axes[2].set_title('TE Difference: Dominant Direction Over Time', fontweight='bold')\n",
    "    axes[2].set_ylabel('TE Difference (bits)')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Mutual Information\n",
    "    axes[3].plot(x_axis, mi_results, label='Mutual Information', color='green', alpha=0.8)\n",
    "    axes[3].set_title('Mutual Information (Baseline)', fontweight='bold')\n",
    "    axes[3].set_ylabel('MI (bits)')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    axes[3].legend()\n",
    "    \n",
    "    # 5. P-values (if provided)\n",
    "    if p_macro_to_stock is not None:\n",
    "        axes[4].semilogy(x_axis, p_macro_to_stock, label='p-value (Macro→Stock)', color='blue', alpha=0.8)\n",
    "        axes[4].semilogy(x_axis, p_stock_to_macro, label='p-value (Stock→Macro)', color='orange', alpha=0.8)\n",
    "        axes[4].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, linewidth=2, label='p = 0.05 (显著性阈值)')\n",
    "        axes[4].fill_between(x_axis, 0, 0.05, alpha=0.1, color='green', label='显著区域 (p < 0.05)')\n",
    "        axes[4].set_title('P-values from Permutation Test\\n零假设: 随机打乱 source 的时间顺序 (permute method)', fontweight='bold')\n",
    "        axes[4].set_ylabel('P-value (log scale)')\n",
    "        axes[4].set_xlabel('Date' if dates is not None else 'Window Index')\n",
    "        axes[4].grid(True, alpha=0.3)\n",
    "        axes[4].legend(fontsize=8)\n",
    "        axes[4].invert_yaxis()  # Lower p-values at top\n",
    "    else:\n",
    "        axes[3].set_xlabel('Date' if dates is not None else 'Window Index')\n",
    "    \n",
    "    # Mark economic events (Question 3)\n",
    "    if dates is not None:\n",
    "        crisis_2008 = pd.Timestamp('2008-09-30')\n",
    "        covid_2020 = pd.Timestamp('2020-03-31')\n",
    "        n_axes_to_mark = 4 if p_macro_to_stock is None else 5\n",
    "        for ax in axes[:n_axes_to_mark]:\n",
    "            if dates[0] <= crisis_2008 <= dates[-1]:\n",
    "                ax.axvline(x=crisis_2008, color='red', linestyle='--', alpha=0.5)\n",
    "            if dates[0] <= covid_2020 <= dates[-1]:\n",
    "                ax.axvline(x=covid_2020, color='purple', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"TE (Macro→Stock): mean={np.nanmean(te_macro_to_stock):.4f}, std={np.nanstd(te_macro_to_stock):.4f}\")\n",
    "    print(f\"TE (Stock→Macro): mean={np.nanmean(te_stock_to_macro):.4f}, std={np.nanstd(te_stock_to_macro):.4f}\")\n",
    "    print(f\"Conditional TE: mean={np.nanmean(cte_results):.4f}, std={np.nanstd(cte_results):.4f}\")\n",
    "    print(f\"Mutual Information: mean={np.nanmean(mi_results):.4f}, std={np.nanstd(mi_results):.4f}\")\n",
    "    if p_macro_to_stock is not None:\n",
    "        p_m2s_arr = np.array(p_macro_to_stock)\n",
    "        p_s2m_arr = np.array(p_stock_to_macro)\n",
    "        valid_p_mask = ~(np.isnan(p_m2s_arr) | np.isnan(p_s2m_arr))\n",
    "        if valid_p_mask.sum() > 0:\n",
    "            print(f\"\\nP-value Statistics (Permutation Test):\")\n",
    "            print(f\"  零假设构造: 随机打乱 source 的时间顺序 (permute method)\")\n",
    "            print(f\"  置换次数: n_perm = 200\")\n",
    "            print(f\"  判定标准: p < 0.05\")\n",
    "            print(f\"  P-value (Macro→Stock): mean={np.nanmean(p_m2s_arr):.4f}, median={np.nanmedian(p_m2s_arr):.4f}\")\n",
    "            print(f\"  P-value (Stock→Macro): mean={np.nanmean(p_s2m_arr):.4f}, median={np.nanmedian(p_s2m_arr):.4f}\")\n",
    "            sig_m2s = (p_m2s_arr[valid_p_mask] < 0.05).sum()\n",
    "            sig_s2m = (p_s2m_arr[valid_p_mask] < 0.05).sum()\n",
    "            print(f\"  显著窗口 (p < 0.05): Macro→Stock: {sig_m2s}/{valid_p_mask.sum()} ({100*sig_m2s/valid_p_mask.sum():.1f}%)\")\n",
    "            print(f\"  显著窗口 (p < 0.05): Stock→Macro: {sig_s2m}/{valid_p_mask.sum()} ({100*sig_s2m/valid_p_mask.sum():.1f}%)\")\n",
    "    \n",
    "    # Period analysis (Question 3)\n",
    "    if dates is not None:\n",
    "        print(\"\\nPeriod Analysis (Question 3):\")\n",
    "        dates_arr = pd.to_datetime(dates)\n",
    "        periods = {\n",
    "            'Pre-2000': (dates_arr < pd.Timestamp('2000-01-01')),\n",
    "            '2000-2008': ((dates_arr >= pd.Timestamp('2000-01-01')) & (dates_arr < pd.Timestamp('2008-09-01'))),\n",
    "            '2008 Crisis': ((dates_arr >= pd.Timestamp('2008-09-01')) & (dates_arr < pd.Timestamp('2010-12-31'))),\n",
    "            'Post-Crisis': ((dates_arr >= pd.Timestamp('2011-01-01')) & (dates_arr < pd.Timestamp('2020-01-01'))),\n",
    "            'COVID-19 Era': (dates_arr >= pd.Timestamp('2020-01-01'))\n",
    "        }\n",
    "        for period_name, mask in periods.items():\n",
    "            if mask.sum() > 0:\n",
    "                te_m2s_period = np.array(te_macro_to_stock)[mask]\n",
    "                te_s2m_period = np.array(te_stock_to_macro)[mask]\n",
    "                valid_mask = ~(np.isnan(te_m2s_period) | np.isnan(te_s2m_period))\n",
    "                if valid_mask.sum() > 0:\n",
    "                    te_m2s_mean = np.nanmean(te_m2s_period)\n",
    "                    te_s2m_mean = np.nanmean(te_s2m_period)\n",
    "                    dominant = \"Macro\" if te_m2s_mean > te_s2m_mean else \"Stock\"\n",
    "                    print(f\"  {period_name}: Dominant={dominant}, TE(M→S)={te_m2s_mean:.4f}, TE(S→M)={te_s2m_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information-Theoretic Analysis: Macro-Finance Causality\n",
      "\n",
      "[1/3] Preparing data...\n",
      "  Data: 429 observations from 1990-02-28 to 2025-10-31\n",
      "\n",
      "[2/3] Running rolling window analysis...\n",
      "Computing 370 rolling windows (n_perm=200)...\n",
      "  Processed 50/370 windows...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main analysis pipeline\"\"\"\n",
    "    print(\"Information-Theoretic Analysis: Macro-Finance Causality\")\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    print(\"\\n[1/3] Preparing data...\")\n",
    "    data = prepare_data()\n",
    "    print(f\"  Data: {len(data)} observations from {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "    \n",
    "    # Step 2: Rolling window analysis\n",
    "    print(\"\\n[2/3] Running rolling window analysis...\")\n",
    "    results = rolling_analysis(data, window_size=60, n_perm=200)\n",
    "    \n",
    "    # Step 3: Visualize results\n",
    "    print(\"\\n[3/3] Visualizing results...\")\n",
    "    plot_results(results)\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
